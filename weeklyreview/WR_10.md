# Weekly Review 10  

## 1. Definitions and Short Answers  

1. In the base-and-limit registers scheme (given at Page 4),  
- What are the **address space** of the three processes?  

> Process at the top: `256000` ~ `200039`,  
> Process at the middle: `300040` ~ `420939`  
> Process at the bottom: `420940` ~ `880000`  

- Who loads the values of the base and limit registers?  

> The operating system is responsible for loading the values of base and limit registers.  

- What happens when a process attempts to address memory location outside its address space?  

> A trap (or an exception) is raised to the operating system monitor due to addressing error.  

2. Does a compiler determine the absolute address of the memory references?  If not, what kind of address does it generate?  

> The compiler does not always determine the absolute address of the memory reference. It generates a relocatable address (address identified by its offset from base address or relocation base).  

3. Which tool (compiler, linker, loader) determines the actual address to access memory for **compile-time binding**?  What must be known in order to perform binding?  

> In compile-time binding, the linker or loader binds the relocatable address (generated by the compiler) to the absolute addresses. However, the memory location must be known as a priori knowledge. For example: in 8051, the code memory location starts at `0H`, so this allows the linker to produce absolute code.  

4. For **load-time binding**, what does the loader actually do?  And what kind of code must the compiler-linker generate?  

> In load-time binding, the loader will calculate the values by adding the base register value (`.BS`) and offset from the relocatable address (generated by the compiler), and fills in the addresses for each memory access instruction.  

5. If **execution-time binding** is used, what assumption can be made about the base address of the process? Who calculates the base address + offset to form the actual memory address, and at what time?  

> In execution-time binding, it is assumed that:  
> - The operating system has already loaded the values for base and limit registers, and  
> - There is a hardware support for base and limit registers and memory management unit (MMU).  
> \
> The hardware translates every reference from virtual addresses to physical addresses (described by `physical = base + offset`) at runtime.  

6. What is the meaning of a **logical address**? 

> A logical address (or also known as virtual address) is an address generated by the CPU. 

7. What is the meaning of a **physical address**?  How is logical address different or same as the physical address for compile-time binding, load-time binding, and execution-time binding.  

> On the other hand, the a physical address is an address seen by the memory unit during memory access.  
> \
> The logical address is the same as the physical address in compile-time binding and load-time binding, while the logical address is remapped to the physical address in execution-time binding.  

8. What does **dynamic loading** mean?  Why is it a good idea?  

> Dynamic loading is a strategy that allows executable code and resources to be loaded into memory at runtime and on demand, rather than loaded all at once during program startup.  
> \
> Advantages of dynamic loading include:  
> - Better memory utilization, unused routines do not take up memory space.  
> - Useful when a large amount of code is used to handle infrequent occurring cases.  

9. What is a problem with the combination of **dynamic loading with static linking**?  

> When dynamic loading is paired with static linking, there could be many redundant copies of the same routines residing in the memory. For instance, three processes loads three different copies of the `printf` function into memory.  

10. What is the mechanism for **dynamic linking**? How does it address the problem of dynamic loading with static linking?  

> Dynamic linking is a method used to link executable program with external libraries or modules at runtime, rather than at compile time. Dynamic linking allows multiple processes to share a single copy of a library in memory, reducing memory usage and facilitating code reuse.  
> \
> It uses a stub to locate the appropriate memory-resident library routine, and replaces itself with the address of the routine, and executes the routine. If the function does not exist, the operating system loads it into memory address space. By using dynamic linking, only at most one copy of library function code resides in the memory and is shared across processes. Hence, dynamic linking addresses the problem arising with dynamic loading with static linking.  

11. What are two partitioning schemes of doing contiguous allocation?  

> There are two partitioning schemes of contiguous allocation:  
> 1. Fixed(-length) partition: each process is loaded into one partition of a fixed size. The degree of multiprogramming is limited by the number of partitions.  
> 2. Variable(-length) partition (also known as dynamic partitioning): each proces is allocated a partition that matches its size.  

12. What is a limitation imposed by **fixed partition scheme** of contiguous allocation?  

> The fixed partition scheme faces several issues:  
> - Internal fragmentation can lead to inefficient use of memory. Internal fragmentation occurs when processes do not fully utilize the allocated partitions.  
> - Limited flexibility and degree of multiprogramming. The fixed size of partitions can limit the size of processes that can be accomodated.  

13. What is a problem caused by **variable partition scheme** of contiguous allocation?  

> The variable partition schemes, also, faces several issues:  
> - External fragmentation can lead to inefficient use of memory. External fragmentation occurs when the total available memory space exists to satisfy a request, but is not contiguous. 
> - More complex management is required.  

14. What are the three common schemes for variable-partition contiguous memory allocation?  Which one is generally faster?  Which ones may need to search the entire list of holes?  

> Three common schemes for variable-partition contiguous memory allocation are:  
> 1. First-Fit  
> &nbsp;&nbsp;&nbsp;Allocate the first hole that is big enough, generally simpler and faster than other schemes.  
> 2. Best-Fit:  
> &nbsp;&nbsp;&nbsp;Allocate the smallest hole that is big enough, produces the smallest leftover hole.  
> 3. Worst-Fit:  
> &nbsp;&nbsp;&nbsp;Allocate the largest hole that is big enough, produces the largest leftover hole (A little counter-intuitive!).  

15. What is the rationale for using **worst-fit**?  

> Worst-Fit produces the largest leftover hole with the hope that the remaining leftover hole can accommodate more incoming processes.  

16. What is **external fragmentation**?  

> External fragmentation is a memory management issue that occurs when free memory blocks in a computer's memory become fragmented, making it difficult to allocate contiguous memory space for new processes or data. This fragmentation can result inefficient memory usage, as there may be enough free memory to satisfy a request.  

17. What is **internal fragmentation**?  

> Internal fragmentation is a memory management issue that occurs when allocated memory partitions in a computer's memory are larger than the actual used amount of memory. This leads to wasted memory space within each allocated block, reducing overall memory efficiency.  

18. In the following two scenarios, identify what kind of fragmentation it is.   

- Fragmentation A  

| 1K used | 2K free | 1K used | 2K free |
| :--: | :--: | :--: | :--: | 

> External fragmentation (free memory is scattered across the memory).  

- Fragmentation B  

| 1.1K used  out of 2K | 1.5K used  out of 2K | 1.2K used  out of 2K | 1.1K used  out of 2K |
| :--: | :--: | :--: | :--: | 

> Internal fragmentation (free memory is allocated within each partition).  

19. **Compaction** may be a way to reduce external fragmentation, but it is not always possible.  What kind of **address binding** (compile time, load time, execution time) is required for compaction to work?  Even if it is supported, under what situation may it still not work correctly?  

> Compaction is a memory management technique used to reduce external fragmentation. It involves rearranging memory contents so that free memory segments become contiguous, making it easier to allocate large blocks of memory to processes. However, execution-time binding is required for compaction to work since the compile-time binding and load-time binding produces absolute addresses that may cause error when the memory content is moved.  
> \
> Compaction is limited by several cases:  
> 1. Not applicable to compile-time binding and load-time binding, unless double-indirect pointers.  
> 2. The process may be tied with memory dependencies, like I/O buffer or shared memory that requires absolute physical address. The relocation mechanism has to be aware of the semantics of pointers, adding much more complexity.  

20. Paging is a way of organizing **non-contiguous** allocation.  What does non-contiguous mean in this case?  

> Non-contiguous allocation, also known as scattered memory allocation, refers to a memory management scheme in which a computer system allocates non-sequential or non-continuous physical memory to processes or data structures.  

21. Does paging use variable partition or fixed partition?  

> Paging uses a fixed partition. a partition can be made up of multiple partitions.  

22. Does paging have internal fragmentation?  external fragmentation?  

> Paging can have internal fragmentation, but avoids external fragmentation because it uses a fixed-size partitioning.  

23. Why does paging tend to have less external fragmentation than contiguous allocation?  

> Paging uses a fixed-size partitioning. Hence, all produced holes are made up of at least one partition that can always be allocated to other processes or data structures. Therefore, paging has much less external fragmentation than contiguous allocation.  

24. What is a **page** and what is a **frame**?  

> A page is a logical unit of memory used by a process, representing a portion of a process' address space. Meanwhile, a frame is a physical unit of memory within the physical memory (RAM), representing a fixed-size block of memory in physical hardware.  

25. To support paging, how should a logical address be divided so that it can be mapped to a physical address? Which part of the address is mapped and which part is the same?  

> Assume that the page size is`2`<sup>`n`</sup>, where `n` is a positive ineteger, and the logical address space is `2`<sup>`m`</sup>, where `m` is a positive integer.  
> \
> The lower `n` bits of the logical address stays the same, and is referred to the page offset `d`. Meanwhile the upper `m - n` bits of the logical address is remapped to the physical address, and is referred to the page number `p` (used as an index into a page table).  

26. What are the advantages and disadvantages of:  
	Express them in terms of page table size (i.e., number of entries) and fragmentation (say which kind fragmentation).
- A smaller page size  

> Advantage: Larger page table (requires more page table entries)  
> Disadvantage: Less severe internal fragmentation (smaller portion of the memory may be affected by internal fragmentation)  

- A larger page size

> Advantage: Smaller page table (requires less page table entries)  
> Disadvantage: More severe internal fragmentation (larger portion of the memory may be affected by internal fragmentation)  

27. What does a page table map from and to?  What does a frame table map from and to?  Do you need one per process or one for the entire system?  

> A page table maps from the the page number `p` in the logical address to the physical address prefix `f`, located in the entry of a page table if exists. Each process has its own page table.  
> \
> Meanwhile, a frame table identifies which frame is available. Each of its entry represents the status of the frame, must include at least the tuple of `(page number, process ID)`.  

28. What are two **registers** that identify a page table?  

> Page-table base register (PTBR) and page-table length register (PTLR) can be used a page table for a process. Page-table base register (PTBR) is a register used as a pointer to the page table in physical memory, while page-table length register (PTLR) refers to the size of the page table.  

29. What does TLB stand for?  Is it hardware or software, and what does it do?  

> Translation Look-Aside Buffer (TLB) is a specialized, high-speed hardware cache used to improve the speed and efficiency of virtual-to-physical address translation.  

30. What happens on a TLB miss?  What happens if all TLB entries are occupied?  

> On a TLB miss, the operating system loads the missing page-table entry into the TLB for faster access next time. If all TLB entries are occupied, one entry must be replaced with the new entry, using the adopted replacement policy (e.g. Least-Recently-Used, abbreviated as LRU).  

31. What is ASID in a TLB entry? Is it mandatory? What are its benefits?  

> Address-Space Identifiers, abbreviated as ASID, is a field within a TLB entry that has a one-to-one (unique) mapping to each process, serves to differentiate between different processes.  
> \
> ASID is optional in a TLB. However, without ASID, the operating system will need to flush out all TLB entries each time a context switch occurs. Hence, every context switch will have a cold start (empty TLB). Every process in the beginning of every context switch will need to repopulate (warm up) the TLB.  
> \
> With ASID in the TLB, the operating system can keep the existing entries in the TLB with the cost of needing to compare the ASID in the entry with the current process at every memory access. Statistically, old processes do not need to repopulate the TLB upon returning to execution.  

32. What is the purpose of bits for indicating **access rights** of a page?  

> The bits used to indicate access rights of a page in virtual memory systems serve the purpose of controlling and enforcing memory protection and access control. For example:  
> - Read (R): If set, it allows read access to the page.  
> - Write (W): If set, it allows write access to the page.  
> - Execute (X): If set, it allows code execution from the page.  
> - No-Execute (NX): If set, it disallows code execution from the page.  
> - and some other access rights.  

33. Why would some pages be marked as **invalid** in a page table entry?  

> A page can be marked as invalid if the page is not in the logical address space of the process.  

34. Why is the reason for using a PTLR (page table length register)? Isn't the size of a page table fixed?  

> Page Table Length Register (PLTR) can be used to save memory for page table since most of the page table entries are unused anyway.  

35. Can **shared memory** between processes be supported?  Do different process need to use the same virtual address?  Do the virtual addresses of the different processes map to the same physical address?  

> Shared memory between processes is supported by mapping different virtual addresses in different processes into one identical physical copy.  

36. How does a **2-level hierarchical page table scheme** divide the logical address into different fields, and what are the steps in looking up the frame number?  

> Consider a 2-level page table in a 32-bit address space.  

| Index of `T`<sub>`o`</sub> | Index of found `T`<sub>`p`</sub> | Page offset |  
| --- | --- | --- |  
> - The highest order bits index into an outer page table `T`<sub>`o`</sub> to find the page-table `T`<sub>`p`</sub> in the page of page-table.  
> - The successive highest order bits index into the page table `T`<sub>`p`</sub> to find the page of data.  
> - The remaining bits are are offset into the page of data to access the data.  

37. How does a **hashed page table** store its entries?  

> The virtual page number is hashes into a page table, with each page table containing the linked list of elements hashed to the same location. Each element of the linked list must contain `(virtual page number, value of the mapped page frame number, pointer to next element)`.  
> \
> This method assumes that not many collisions will happen and uses linear search to search through the linked list.  

38. Is a **clustered page table** a form of a hashed page table? How is it more economical?  

> A clustered page table is a variation of a hashed page table for 64-bit addresses. This method assumes that the address space is spare, where logical memory is sparse but physical memory references are non-contiguous and scattered.  
> \
> However, instead of using a linked list data structure as the hashed value, the clustered page table uses a page table as the hashed value. This method uses less number bits in the hash key and the remaining bits can be used as an offset in the page table described by the hashed value.  
> \
> This approach is more economical because the hash table shrinks in size. Furthermore, each memory access in logical address space, by the principle of locality, is often sequential or clustered. Hence, clustered memory access in logical address space maps to the same cluster in the page table described by the hashed value.  