# Operating Systems Weekly Review 11  

## 1. Definitions and Short Answers  

1. Of the several benefits of virtual memory, why is partial program loading a good idea?  

> With partial program loading, we see some advantages:  
> - Programs are not constrained by limited physical memory.  
> - More programs can reside in the memory, increasing throughput and CPU utilization with no increase in response time or turnaround time.  
> - Less I/O is needed to load or swap programs into memory.  

- How does it improve performance (run faster)  

>  There is less I/O needed to load or swap programs in and out of the memory since some part of the programs reside in the memory.  

- How does it use less physical memory?  

> Only necessary parts of a program can reside in the memory, while others can be picked up from the disk when needed. Thus, a program requires less physical memory than its entire program size.  

- How does it allow more processes to run at the same time?  

> Because only the necessary parts of a program reside in the memory, there are more physical space available. The physical memory can hold more parts of other processes. With more processes in the memory, more processes can run at the same time.  

2. Another benefit of virtual memory is that it allows the logical address space to be larger than physical. Why is this a good idea?  

> Allowing the logical address space to be larger than the physical address space provides an abstraction of an entire memory space is allocated to each of the user programs. Hence, programs (and programmers) are not constrained by limited physical memory.  

3. What is the difference between **paging** and **swapping**?  

> The main difference of paging and swapping is in the granularity. Paging refers to the movement of a page from the disk to the memory (page in) and from the memory to the disk (page out). On the other hand, swapping refers to the movement of an entire process memory from the disk to the memory (swap in) and an entire process memory from the memory to the disk (swap out).  

4. What does **demand paging** mean?  What are its benefits compared to non-demand-paging?  

> Demand paging is a memory management technique used to load only specific portions of a program into memory when they are needed, or on-demand.  
> \
> Advantages of demand paging when compared to non-demand-paging are:  
> - Less unnecessary I/O is needed,  
> - Less physical memory needed,  
> - Faster response (there is no need to wait for the entire program to swap in)  
> - More user programs.  

5. When the **valid-invalid bit** in the page table entry is `'v'`, what does it mean for demand paging?  

> When the page table entry has the valid bit`'v'`, the page is in memory. A memory access to a page table entry with `'v'` means that it can safely proceed.  

6. When the valid-invalid bit is `'i'` in the page table entry, what are two possibilities? How does the OS handle them?  

> When the page table entry has the invalid bit `'v'`, the page is either:  
> - An invalid memory reference (out-of-bound). The memory access must be aborted, trap to the operating system for invalid memory access.  
> - Valid memory reference but the page is not in memory. The memory access traps to the operating system for a page fault and the page fault handler loads the missing page into memory.  

7. What happens on a page fault in demand paging? Does the OS or CPU handle it like a regular interrupt?  What is a subtle but important difference in how user code is resumed?  

> When a page fault occurs, there will be a trap to the operating system and to the page fault handler. The procedure for a page fault is:  
> 1. The OS finds a free frame from the free-frame list or forcefully kick out an existing frame.  
> 2. The OS reads the missing page into the free frame via scheduled disk operation.  
> 3. The OS updates the tables to indicate the page is in the memory, setting the valid bit field to `'v'`.  
> 4. The OS restarts the instruction that causes the page fault.  
> \
> Unlike a regular interrupt, the executed instruction upon return is not the next instruction, but the instruction that causes the page fault itself.  

8. What does **zero-fill-on-demand** mean? When is it used and why?  

> Zero-fill-on-demand is a memory management technique used in virtual memory systems to allocate and initialize free memory pages only when a process or program obtains it. The main purpose of flushing out the existing memory content before allocating the page to other processes is to disallow processes from seeing the leftover data from other processes.  

9. Of the costs of page fault, which has the highest cost?  
- Servicing the interrupt  
- Restart the process  
- Page-in, page-out   

> Page-in and page-out are the most expensive.  

10. What is **copy-on-write** and how does it help make `fork()` run more efficiently?  

> Copy-on-write is a memory management technique used to optimize the use of memory and resources when copying data. It allows multiple processes or users to share the same data in memory until one of them attempts to write on it. When a modification occurs, a separate copy of the data is created for the process making the change.  
> \
> After a `fork()`, child processes possess the exact copy of the memory contents of the parent processes. With copy-on-write, both parent and children an share the same pages in memory after `fork()` until just before one of them modifies the content. This technique saves the CPU time from performing many unnecessary I/O involved in copying memory contents, increasing the performance of `fork()`.  

11. Does `vfork()` use copy-on-write?  How does it work for a shell to launch a process?  

> `vfork()` creates a child process and suspends the parent process. Moreover, it does not use copy-on-write and directly modifies the parent's memory contents. It can be used to implement command-line shells to launch a process. The child process calls `exec()` after `vfork()`.  

12. What does a **frame allocation algorithm** decide?  

> A frame allocation algorithm is an essential algorithm in a computer's memory management system used to determine how many physical memory frames to allocate to a process in a way that optimizes the memory utilization and ensures efficient operation.  

13. What does a **page replacement algorithm** decide?   

> A page replacement algorithm is an essential algorithm in a computer's memory management system used to determine which physical memory frame to replace when a page fault occurs or the operating system requires a free frame.  

14. What is the **Belady's Anomaly**?  

> Belady's Anomaly is a phenomenon in the study of page replacement algorithms in virtual memory management. It refers to a counter-intuitive behavior commonly observed in FIFO (First-In-First-Out) page replacement algorithm where increasing the number of available page frames can lead to an increase in the number of page faults rather than a decrease.  
> \
> Some algorithms do not exhibit Belady's Anomaly, including: Least-Recently-Used (LRU) and optimal page replacement algorithm (designed by Belady himself).  

15. What is the **optimal page replacement algorithm**? Can it be implemented exactly?  

> Optimal page replacement algorithm is a theoretical page replacement algorithm that makes the best possible decision regarding which page to replace when a page fault occurs or the operating system requires a free frame. It replaces the page thaat will not be used for the longest period of time. However, this algorithm cannot be implemented because it assumes a perfect knowledge of the future page references, which is not possible in practice.  

16. What is a good page replacement algorithm in practice?  

> Least-Recently-Used (abbreviated as LRU) is a generally good and frequently used algorithm in practice.  

17. To implement LRU,  
- What does `Counter` store, when is it updated, and how is it used?  

> Every page entry has a `Counter`. A clock is incremented for every memory reference. For each referenced page, the `Counter` is assigning the current value of the clock. When a page needs to be selected as a victim, the page with the lowest value of `Counter` will be chosen.  

- How does a stack algorithm work when the page being referenced is already on the stack?  

> A stack algorithm can be used to implement the least-recently-used (LRU) algorithm without clock and counters. The stack will consist of page numbers in a doubly-linked list stack data structure. The top defines the most recently used page number.  
> \
> When a page being referenced is not on the stack, then push its page number onto the top of the stack. On the other hand, if a page being referenced is already on the stack, remove the page number from the stack and re-push it on the top of the stack. However, although stack removes the need of a search algorithm, this procedure requires updating six pointers, making it an expensive operation.  

18. Of LRU approximation algorithms,  
- In **single reference bit algorithm**, what page gets replaced?  

> The page with the reference bit equals to `0` gets replaced.  

- In **additional reference bits algorithm**, how are multiple reference bits for each page maintained?  

> Every page consists of some reference bits. The operating system will perform sampled update every few milliseconds, where it right shifts the reference bit. If the page has been referenced, then `1` will be placed as the most significant bit (MSB). Otherwise, `0` will be placed as the most significant bit (MSB). The page with the smallest value of reference bit will be picked as a victim.  

- How does **second-chance algorithm** decide what page to replace? What update to reference bit does it do under what condition?  

> Second-chance algorithm is usually implemented as a FIFO (First-in-first-out) algorithm. The algorithm maintains a pointer in circular buffer. When deciding which page to replace, the pointer will traverse the pages sequentially. If the pointer encounters a page with reference bit `1`, then set the reference bit to `0` and continue traversing. Otherwise, if the pointer encounters a page with reference bit `0`, then the page will be chosen as the victim. Moreover, the reference bit will also be set to `1` if the page is referenced.  

- In **enhanced second chance algorithm**, what are the four combinations in order of replacement priority?  

> Enhanced second chance algorithm maintains an ordered pair `(reference, modify)` for every page, with the priority below (ascending priority):  
> 1. `(0, 0)`: Neither used nor modified (best to replace).  
> 2. `(0, 1)`: Not recently used but modified (must perform write when swapped out).  
> 3. `(1, 0)`: Recently used but not modified (will probably be used again).  
> 4. `(1, 1)`: Recently used and modified (least desirable to replace, must perform write when swapped out).  

19. In Counting-based algorithms,  
- What is the issue with least-frequently used (LFU) algorithm?  

> Counting-based algorithm maintains a counter of the number of references of each page. In Least-Frequently-Used algorithm (LFU), pages with high number of references that are no longer used in the future have a large count but cannot get replaced easily. They occupy some space and reduce performance.   
> \
> A solution to this is by shift count over time (or by implementing exponential decaying average).  

- What is the rationale behind most-frequently used (MFU) algorithm?  

> It can be argued that the the page with smallest count was the page that was just brought into the memory and has yet to be used.  

20. In Page-buffering,   
- How does it make sure there are always free frames available by the time a page fault occurs?  

> To make sure that there are always free frames available by the time a page fault occurs, the operating system can select a victim page to kick out (and write back to disk if necessary) and add the frame into the free list when it is idle.  

- An extended version keeps a list of modified pages.  What is the purpose?  

> When backing store is idle, the operating system can write pages back into the disk and set the page to non-dirty. If a page is non-dirty, evicting such a page will not incur any I/O overhead.  

- Another extended version is to retain the frame contents even when the frame is put on the free frame list.  What is the purpose?  

> If the process references and obtains the same frame it releases, there is no need to reload contents again from disk, saving some I/O overhead.  

21. What are two schemes for **fixed allocation of frames** to processes?  

> Two schemes of fixed allocation of frames are:  
> 1. Equal allocation  
> &nbsp;&nbsp;&nbsp;&nbsp;Each process obtains an equal number of frames, regardless of its size.  
> 2. Proportional allocation  
> &nbsp;&nbsp;&nbsp;&nbsp;Each process obtains frames with number proportional to its size. The number of frames may be allocated dynamically with respect to the degree of multiprogramming and process sizes (which change over time).  

22. In **priority allocation of frames**, upon page fault, how would the operating system select victims to reflect priority of processes?  

> In priority allocation of frames, processes are assigned with priorities. Higher priority processes are likely to obtain more frames, while lower priority processes are likely to loss frames. Upon a page fault, if there is no free frame available, then the operating system will select a victim from a process with lower priority.  

23. Between **global replacement** and **local replacement**,  
- What are the advantages and disadvantages of global replacement?  

> With global replacement policy, a process can take a frame from other processes. The advantages of such policy are:  
> 1. Greater throughput  
> 2. Better utilization  
> The disadvantage of such policy is:  
> 1. Less predictable execution time (some processes may lose frames and execute much slower)  

- What are the advantages and disadvantages of local replacement?  

> With local replacement policy, a process can only take a frame from its own set of allocation frames. The advantage of such policy is:  
> 1. More consistent per-process performance  
> The disadvantage of such policy is:  
> 1. Possibly underutilized memory (process frequently swaps in and out pages to operate)  

24. What is the difference between a **major page fault** and a **minor page fault**?  

> A major page fault occurs when the operating system must fetch data from the disk (or backing store) nto a free frame in the memory because the referenced page is not currently in memory. Meanwhile, a minor page fault occurs when the referenced page still remains in the memory but is on the free list and not yet assigned to another page. In this case, the operating system only needs to update the page table.  

25. What is the purpose of a **reaper**?  

> A reaper is a kernel routine that reclaim pages. It is invoked when the amount of free memory drops below a minimum threshold until the amount of free memory reaches a meximum threshold (Schmitt trigger). When invoked, a reaper reclaims and adds frames to the free frame list.  

26. What is the meaning of **thrashing**?  What causes thrashing?

> Thrashing is the phenomenon in which a computer's performance deteriorates significantly as a result of excessive and inefficient paging or swapping of pages between physical memory and disk. Thrashing occurs when a system spends a majority of its time swapping pages in and out of memory, instead of performing some useful work.  
> \
> Thrashing occurs when the total size of locality exceeds the total memory size. It is caused by a vicious cycle:  
> 1. When a process does not have enough frames, pages were swapped in and out frequently to ensure progress. Hence, the majority of its operations are I/O.  
> 2. When many processes are not getting enough frames, all of them are waiting for I/O. The operating system views this as a sign to increase the degree of multiprogramming since the CPU is underutilized.  
> 3. More processes are initiated and old processes lose even more frames, removing already insufficient resources.  

27. How does the **working set model** approximate locality? And what condition implies thrashing?  

>  A working set model is a memory management concept that helps approximate the temporal locality of reference in a program's memory access patterns. The operating system records each memory access within a working-set window, calculating its working-set size (the number of distinct referenced pages).  
> \
> When the total demand frames (or the total working-set size across processes) exceed the number of available frames, then thrashing is taking place. The operating system can suspend or swap out a process to solve thrashing.  

28. In the page-fault frequency (PFF) strategy, what does it mean when the page fault rate is too high and what does the OS do? Too low?  

> The operating system must establish an acceptable page-fault frequency (PFF). When the actual page-fault frequency exceeds the upper bound, the operating systems must add free frames to the process. On the other hand, if the actual page-fault frequency is below the lower bound, the operating system can take frames from the process.  

29. What is different between physical memory allocation for kernel vs. for user process?  

> Some of the physical memory allocation for kernel must be in contiguous manner. Otherwise, another abstraction layer for remapping is needed.  
> \
> On the other hand, the physical memory allocation for user process is logically contiguous, but not physically contiguous. The operating system takes care of the mapping.  

30. How does the buddy system decide how much memory to allocate to serve a request?  What are its advantages and disadvantages?  

> The buddy system allocates memory with the size of the smallest power of two that is larger than or equals to the requested memory size.  
> \
> The advantage of buddy system is that it could quickly coalesce (or combine) unused chunks into a larger chunk. On the other hand, the disadvantage of the buddy system is that internal fragmentation might occur in such case.  

31. In SLAB allocation,  
- What is a slab and how big does it have to be? What does a slab contain?  

> A slab is one or more physically contiguous pages. It should be big enough to contain one or more instances of a given type of kernel data structure.  

- How many caches are there?  What does a cache contain?  

> A cache is a collection of slabs that store objects of the same kernel data structure. There exists one cache for each unique type of kernel data structure, for instance: file descriptor, semaphores, etc.  
> \
> Initially, cache is filled with objects that are marked as free. When structures are stored, the object is marked as used. When a slab of given type is full of used objects, allocate the next object from an empty slab in the cache. If no empty slabs are available, allocate new slab to the cache.  

- Why is there **no fragmentation** in slab scheme?  

> The reasons why slab scheme has no fragmentation are:  
> 1. The granularity of each slab is dedicated to a specific object size. Because the objects are of a fixed size, there is no internal fragmentation.  
> 2. The slab scheme also reuse and recycles objects. When an object is deallocated, it returns to its corresponding slab, waiting for new allocations. The memory is not freed, therefore there can be no holes that may cause external fragmentation.  
> \
> Another advantage to slab allocation is fast memory request due to slab's method of reusing and recycling objects.  